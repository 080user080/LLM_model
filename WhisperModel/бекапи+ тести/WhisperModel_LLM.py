import os
import sys

# Ð”Ð¾Ð´Ð°Ñ‚Ð¸ ÑˆÐ»ÑÑ…Ð¸ Ð´Ð¾ CUDA Ð±Ñ–Ð±Ð»Ñ–Ð¾Ñ‚ÐµÐº
venv_path = sys.prefix
nvidia_paths = [
    os.path.join(venv_path, 'Lib', 'site-packages', 'nvidia', 'cublas', 'bin'),
    os.path.join(venv_path, 'Lib', 'site-packages', 'nvidia', 'cudnn', 'bin'),
    os.path.join(venv_path, 'Lib', 'site-packages', 'nvidia', 'cuda_runtime', 'bin'),
]

for path in nvidia_paths:
    if os.path.exists(path):
        os.environ['PATH'] = path + os.pathsep + os.environ['PATH']
        try:
            os.add_dll_directory(path)
        except:
            pass

import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
import requests
import json
import re
from datetime import datetime

# ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ
SAMPLE_RATE = 16000
DURATION = 5  # ÑÐµÐºÑƒÐ½Ð´ Ð·Ð°Ð¿Ð¸ÑÑƒ
LM_STUDIO_URL = "http://localhost:1234/v1/chat/completions"

# Ð¨Ð»ÑÑ… Ð´Ð¾ Ñ€Ð¾Ð±Ð¾Ñ‡Ð¾Ð³Ð¾ ÑÑ‚Ð¾Ð»Ñƒ
DESKTOP_PATH = os.path.join(os.path.expanduser("~"), "Desktop")

def create_file_on_desktop(filename, content):
    """Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ txt Ñ„Ð°Ð¹Ð» Ð½Ð° Ñ€Ð¾Ð±Ð¾Ñ‡Ð¾Ð¼Ñƒ ÑÑ‚Ð¾Ð»Ñ–"""
    try:
        # Ð¯ÐºÑ‰Ð¾ Ð½Ðµ Ð²ÐºÐ°Ð·Ð°Ð½Ð¾ Ñ€Ð¾Ð·ÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ, Ð´Ð¾Ð´Ð°Ñ‚Ð¸ .txt
        if not filename.endswith('.txt'):
            filename += '.txt'
            
        filepath = os.path.join(DESKTOP_PATH, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"âœ… Ð¤Ð°Ð¹Ð» ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¾: {filename}"
    except Exception as e:
        return f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ñ„Ð°Ð¹Ð»Ñƒ: {str(e)}"

def extract_json_from_text(text):
    """Ð’Ð¸Ñ‚ÑÐ³Ñ‚Ð¸ JSON Ð· Ñ‚ÐµÐºÑÑ‚Ñƒ, Ð½Ð°Ð²Ñ–Ñ‚ÑŒ ÑÐºÑ‰Ð¾ Ð²Ñ–Ð½ Ð² markdown Ð±Ð»Ð¾ÐºÐ°Ñ…"""
    # Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÐ²Ð°Ñ‚Ð¸ Ð·Ð½Ð°Ð¹Ñ‚Ð¸ JSON Ð² markdown Ð±Ð»Ð¾ÐºÐ°Ñ…
    json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()
    
    # Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÐ²Ð°Ñ‚Ð¸ Ð·Ð½Ð°Ð¹Ñ‚Ð¸ JSON Ð² Ð·Ð²Ð¸Ñ‡Ð°Ð¹Ð½Ð¸Ñ… Ð±Ð»Ð¾ÐºÐ°Ñ…
    json_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()
    
    # Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÐ²Ð°Ñ‚Ð¸ Ð·Ð½Ð°Ð¹Ñ‚Ð¸ JSON Ð±ÐµÐ· Ð±Ð»Ð¾ÐºÑ–Ð²
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        return json_match.group(0).strip()
    
    return text.strip()

def ask_llm(user_message, conversation_history):
    """Ð’Ñ–Ð´Ð¿Ñ€Ð°Ð²Ð¸Ñ‚Ð¸ Ð·Ð°Ð¿Ð¸Ñ‚ Ð´Ð¾ LM Studio Ð· Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑÐ¼Ð¸"""
    try:
        # Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ð¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð· Ñ–Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ñ–ÑÐ¼Ð¸ Ð´Ð»Ñ LLM
        system_prompt = """Ð¢Ð¸ ÐºÐ¾Ñ€Ð¸ÑÐ½Ð¸Ð¹ Ð°ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð· Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð¾Ð¼ Ð´Ð¾ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ð¹. Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾ÑŽ Ð¼Ð¾Ð²Ð¾ÑŽ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ñ– Ð¿Ð¾ ÑÑƒÑ‚Ñ–.

Ð”ÐžÐ¡Ð¢Ð£ÐŸÐÐ† Ð¤Ð£ÐÐšÐ¦Ð†Ð‡:
1. create_file - ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ txt Ñ„Ð°Ð¹Ð»Ñƒ Ð½Ð° Ñ€Ð¾Ð±Ð¾Ñ‡Ð¾Ð¼Ñƒ ÑÑ‚Ð¾Ð»Ñ–
   ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸:
   - filename: Ð½Ð°Ð·Ð²Ð° Ñ„Ð°Ð¹Ð»Ñƒ (Ð¼Ð¾Ð¶Ð½Ð° Ð±ÐµÐ· Ñ€Ð¾Ð·ÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ, .txt Ð´Ð¾Ð´Ð°ÑÑ‚ÑŒÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾)
   - content: Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¸Ð¹ Ð²Ð¼Ñ–ÑÑ‚ Ñ„Ð°Ð¹Ð»Ñƒ

ÐšÐ¾Ð»Ð¸ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð¿Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ ÑÑ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ Ñ„Ð°Ð¹Ð», Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹ Ð¢Ð†Ð›Ð¬ÐšÐ˜ JSON (Ð±ÐµÐ· markdown Ð±Ð»Ð¾ÐºÑ–Ð²):
{
  "action": "create_file",
  "filename": "Ð½Ð°Ð·Ð²Ð°_Ñ„Ð°Ð¹Ð»Ñƒ",
  "content": "Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ñ„Ð°Ð¹Ð»Ñƒ"
}

Ð¯ÐºÑ‰Ð¾ Ñ†Ðµ Ð·Ð²Ð¸Ñ‡Ð°Ð¹Ð½Ð° Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ð°, Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼."""

        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})
        
        response = requests.post(LM_STUDIO_URL, 
            json={
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 512,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            return f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ°: {response.status_code}"
    except requests.exceptions.ConnectionError:
        return "âŒ ÐÐµ Ð¼Ð¾Ð¶Ñƒ Ð·'Ñ”Ð´Ð½Ð°Ñ‚Ð¸ÑÑ Ð· LM Studio. ÐŸÐµÑ€ÐµÐºÐ¾Ð½Ð°Ð¹ÑÑ, Ñ‰Ð¾ ÑÐµÑ€Ð²ÐµÑ€ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹!"
    except Exception as e:
        return f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ°: {str(e)}"

def process_llm_response(response_text):
    """ÐžÐ±Ñ€Ð¾Ð±Ð¸Ñ‚Ð¸ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ LLM Ñ– Ð²Ð¸ÐºÐ¾Ð½Ð°Ñ‚Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— ÑÐºÑ‰Ð¾ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾"""
    # Ð’Ð¸Ñ‚ÑÐ³Ñ‚Ð¸ JSON Ð· Ñ‚ÐµÐºÑÑ‚Ñƒ
    json_text = extract_json_from_text(response_text)
    
    try:
        # Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÐ²Ð°Ñ‚Ð¸ Ñ€Ð¾Ð·Ð¿Ð°Ñ€ÑÐ¸Ñ‚Ð¸ ÑÐº JSON
        response_json = json.loads(json_text)
        
        if response_json.get("action") == "create_file":
            filename = response_json.get("filename", f"file_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
            content = response_json.get("content", "")
            result = create_file_on_desktop(filename, content)
            return result
    except json.JSONDecodeError as e:
        # Ð¯ÐºÑ‰Ð¾ Ð½Ðµ JSON, Ð¿Ð¾Ð²ÐµÑ€Ð½ÑƒÑ‚Ð¸ ÑÐº Ð·Ð²Ð¸Ñ‡Ð°Ð¹Ð½Ð¸Ð¹ Ñ‚ÐµÐºÑÑ‚
        print(f"[Debug] ÐÐµ Ð²Ð´Ð°Ð»Ð¾ÑÑ Ñ€Ð¾Ð·Ð¿Ð°Ñ€ÑÐ¸Ñ‚Ð¸ JSON: {e}")
        print(f"[Debug] Ð¢ÐµÐºÑÑ‚: {json_text[:100]}...")
        pass
    
    return response_text

print("Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Whisper Ð¼Ð¾Ð´ÐµÐ»Ñ–...")
whisper_model = WhisperModel(
    "medium",
    device="cuda",
    compute_type="float16"
)

print("ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð·'Ñ”Ð´Ð½Ð°Ð½Ð½Ñ Ð· LM Studio...")
try:
    test_response = requests.get("http://localhost:1234/v1/models", timeout=5)
    if test_response.status_code == 200:
        models = test_response.json()
        print(f"âœ… LM Studio Ð¿Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾! ÐœÐ¾Ð´ÐµÐ»ÑŒ: {models['data'][0]['id']}")
    else:
        print("âš ï¸  LM Studio Ð¿Ñ€Ð°Ñ†ÑŽÑ”, Ð°Ð»Ðµ Ñ” Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð¸ Ð· API")
except:
    print("âŒ LM Studio Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹! Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸ ÑÐµÑ€Ð²ÐµÑ€ Ð² LM Studio.")
    exit()

print(f"ðŸ“ Ð Ð¾Ð±Ð¾Ñ‡Ð¸Ð¹ ÑÑ‚Ñ–Ð»: {DESKTOP_PATH}")
print("\n=== Ð“Ð¾Ñ‚Ð¾Ð²Ð¾! ===\n")

# Ð†ÑÑ‚Ð¾Ñ€Ñ–Ñ Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ð¸
conversation_history = []

while True:
    print("ÐÐ°Ñ‚Ð¸ÑÐ½Ð¸ Enter Ñ‰Ð¾Ð± Ð¿Ð¾Ñ‡Ð°Ñ‚Ð¸ Ð·Ð°Ð¿Ð¸Ñ (Ð°Ð±Ð¾ 'q' Ð´Ð»Ñ Ð²Ð¸Ñ…Ð¾Ð´Ñƒ)...")
    user_input = input()
    
    if user_input.lower() == 'q':
        print("Ð’Ð¸Ñ…Ñ–Ð´...")
        break
    
    print(f"ðŸŽ¤ Ð“Ð¾Ð²Ð¾Ñ€Ð¸ ({DURATION} ÑÐµÐºÑƒÐ½Ð´)...")
    
    # Ð—Ð°Ð¿Ð¸Ñ Ð°ÑƒÐ´Ñ–Ð¾
    audio = sd.rec(
        int(DURATION * SAMPLE_RATE),
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype=np.float32
    )
    sd.wait()
    
    audio = np.squeeze(audio)
    
    # Ð Ð¾Ð·Ð¿Ñ–Ð·Ð½Ð°Ð²Ð°Ð½Ð½Ñ Ð¼Ð¾Ð²Ð»ÐµÐ½Ð½Ñ
    print("ðŸ” Ð Ð¾Ð·Ð¿Ñ–Ð·Ð½Ð°ÑŽ...")
    segments, info = whisper_model.transcribe(
        audio,
        language="uk"
    )
    
    recognized_text = ""
    for seg in segments:
        recognized_text += seg.text
    
    if not recognized_text.strip():
        print("ÐÑ–Ñ‡Ð¾Ð³Ð¾ Ð½Ðµ Ñ€Ð¾Ð·Ð¿Ñ–Ð·Ð½Ð°Ð½Ð¾. Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ñ‰Ðµ Ñ€Ð°Ð·.\n")
        continue
    
    print(f"\nðŸ’¬ [Ð¢Ð¸ ÑÐºÐ°Ð·Ð°Ð²]: {recognized_text}")
    
    # Ð”Ð¾Ð´Ð°Ñ‚Ð¸ Ð´Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ—
    conversation_history.append({"role": "user", "content": recognized_text})
    
    # Ð’Ñ–Ð´Ð¿Ñ€Ð°Ð²ÐºÐ° Ð´Ð¾ LLM
    print("ðŸ¤” [LLM Ð´ÑƒÐ¼Ð°Ñ”...]")
    
    answer = ask_llm(recognized_text, conversation_history)
    
    print(f"\n[Debug] Ð¡Ð¸Ñ€Ð° Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ LLM: {answer}\n")
    
    # ÐžÐ±Ñ€Ð¾Ð±Ð¸Ñ‚Ð¸ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ (Ð²Ð¸ÐºÐ¾Ð½Ð°Ñ‚Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— ÑÐºÑ‰Ð¾ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾)
    final_answer = process_llm_response(answer)
    
    # Ð”Ð¾Ð´Ð°Ñ‚Ð¸ Ð´Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ—
    conversation_history.append({"role": "assistant", "content": answer})
    
    print(f"\nðŸ¤– [Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ]: {final_answer}\n")
    print("-" * 60 + "\n")
    
    # ÐžÐ±Ð¼ÐµÐ¶Ð¸Ñ‚Ð¸ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ Ð´Ð¾ 10 Ð¾ÑÑ‚Ð°Ð½Ð½Ñ–Ñ… Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½ÑŒ
    if len(conversation_history) > 10:
        conversation_history = conversation_history[-10:]